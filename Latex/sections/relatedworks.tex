
\chapter{Related works}
\label{ch:relatecworks}
As far as I have found there hasn't been a lot of published work into estimating the quality of spoken language translation so far. The closest to what this thesis is doing has been done by Le et al. \cite{le2016automatic}. 
However they use a supervised approach that is based on Word confidence, so the classification of the confidence is on a word basis rather than sequence basis. Also, the translation step of the cascaded model they used is a statistical machine translation model instead of a neural machine translation model, which is no longer the state of the art machine translation, as neural machine translation models have gotten a lot better at translation in the years since the paper came out. 

Negri et al.\cite{negri-etal-2014-quality} take look at black box ASR quality estimation but have a small amount of white box quality estimation. Their white box methods are not designed for end-to-end ASR models, but they have used different features or information from both the acoustic and language models, and have combined information from both parts of the ASR models to be used in the regression part of the estimator. In their experiments they have shown that the regression based on white box features used to estimate the WER show quite the improvements over the baseline and the regression based on the black box features. 

Analysing uncertainties with the help of dropout on Automatic Speech Recognition systems has been tries by Vyas et al. \cite{8683086}. They look at uncertainty quantification, in particular WER estimation with the help of dropout. The experiments in this are done with ASR models that have an acoustic and a language model. They tried it with 2 different acoustic models. One is an HMM-DNN ASR model, another one is a Connectionist Temporal Classification acoustic model as the Language model they used a trigram LM. The dropout is only enabled on the acoustic models. 
The WER estimation metrics they propose in this make use of a non-dropout enabled transcript and N dropout enabled transcripts. They then calculate the pairwise edit distances between dropout transcripts, sort them in descending order, and take the mean of the top K of those edit distances, as well as the mean of the length of the decoding of those top K edit distances. As a reference they looked at N-best hypothesis of the decoding lattice to estimate the WER with the same estimation metrics. This has showed that this delivers good results for estimating WER on these kinds of of ASR models. 

In a similar vein to Negri et al.\cite{negri-etal-2014-quality} there have been works into predicting the WER. Fe-WER \cite{park2023fastworderrorrate} is an improved version of the e-WER3 \cite{e-wer3}, which is a system to estimate the WER. It mainly improves computation performance without degrading the estimation performance. 
The e-WER3 system in turn is an improved version of the e-WER2 \cite{ali2020worderrorrateestimation}, which is a monolingual system, and e-WER \cite{Ali2018WordER}, which compares both black and white box methods to estimate the WER. The e-WER3 is a framework for estimating multilingual ASR, which takes the audio and the automatic transcription of the ASR model to estimate the WER for that transcription. This works in a black box approach. A further improved version of the Fe-WER has been proposed by Park et al. \cite{park2024automaticspeechrecognitionsystemindependent} where they also employed different Hypothesis generation strategies. 

As mentioned in the introduction the Unsupervised Quality Estimation For Neural Machine translation paper \cite{fomicheva2020unsupervised} is the basis of most of this thesis. It takes a look at different ways to estimate quality estimation in machine translation, all of those in an unsupervised glass box manner. This means that all scores are based on data from within the model that can be gathered during inference and they do not need any reference data or other human input to create those scores in the first place. In addition to the metrics described in \autoref{ch:methods}, which are used in this thesis, they also looked at possible attention based metrics but found that this approach requires Direct Assessment to find the best head/layer which makes it not fully unsupervised. 

Other works that have looked at quality estimation in translation include perturbation-based quality estimation of machine translation \cite{dinh2023perturbationbasedqeexplainableunsupervised}, which looks at word-level quality estimation of text translation on black box systems in an unsupervised manner by changing parts of the input text.  
Quality estimation without human-labeled data \cite{tuan-etal-2021-quality} explores generating synthetic data for quality estimation that can be used for word and sentence level estimation. 
Unbabels' entry for the WMT19 Translation Quality Estimation task \cite{kepler-etal-2019-unbabels}, several submissions to the WMT19s Quality Estimation task (which can be found in \cite{fonseca-etal-2019-findings}), as well as Sun et al.'s Exploratory Study on Multilingual Quality Estimation \cite{sun-etal-2020-exploratory} use Direct Assessment or post-edit distance during training to estimate the quality. 
However such DA annotations are rare and really resource intensive to produce for spoken language, which is why this thesis focuses on unsupervised approaches. 