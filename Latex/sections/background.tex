\chapter{Background}
This chapter explains the methods and concepts these are seperated in 2 parts one for basic knowledge and 

\section{Basic Knowledge}
Important knowledge for the contents of the thesis mainly the what translation, automatic Speech recognition and dropout


\subsection{Automatic Speech Recognition}
Automatic speech recognition systems or short ASR systems are systems that recognise and transcribe spoken language. 
%TODO

\subsection{Translation}
Translation is the practice of translating text or language from one language into another language. This can be done by hand by a human or in a very statical approach where a dictionary is used to directly translate the text 
%TODO
\subsubsection{Speech translation}
Speech translation or spoken language translation is simmilar to the regular translation but it has, like the name says, spoken language as the basis instead of text. 
%TODO


\subsection{Dropout}
- has been utilised in DNN to measure uncertainty
- mainly in Deep architectures but also in Auto-encoders \cite{}
-in ASR it has been tried here  \cite{8683086}
- in ASR it mainly stems from noisy audio 
%TODO


\subsection{Transcription probability}
the probability that the ASR component transcribes the audio to this sequence of text
%TODO


\subsection{Translation probability}
- the probability that the model generates the sequence $y = y_1, y_2 \dots y_n$ for the input $x=x_1, x_2 \dots x_n$
norm the translation probabilty over the length of the transcribed and translation 
%TODO
\subsection{Pearsoncorrelation}
method to see how correlated 2 sets of values are
is 1 if it's correlated and -1 if it's inversely correlated 
if it's 0 the sets are not correlated at all
%TODO

\section{Models}
\subsection{Decoder-Encoder}
-Encoder-Decoder Models are Models that contain a Encoder, lorem ypsum 
and a Decoder, lorem ypsum
-Encoder-Decoder Achitecture is used for Sequence-to-Sequence models
%TODO expand, add gaphics add citations 

\subsection{Transformer}
Neural Network architecture that was first introduced in the paper Attention is all you need \cite{vaswani2023attentionneed} that makes use of selfattention mechanisms. it has a Encoder-Decoder structure
%TODO

\subsection{ Cascaded Models}
Cascaded Speech translation Models consist of 2 parts a part that is resposnibe for transcribing the audio, which is usually done with an ASR model, and a part that is responisble for translating the resulting transcription, which is done with neural machine translation or statistical machine translation. 
%TODO citation and expand
\subsection{End-to-End Models}
End-to-End Speech translation models do not have the explicit split between the Automatic Speech Recognition model and the translation, this means that such a model gets audio as an input and outputs the text in the target language. 
%TODO citations, different architectures and expand
\subsection{Whisper}
Whisper is a multilingual multitask Model that is focused on speech processing and was proposes in the Robust Speech Recogintion via Large-Scale Weak Supervsion paper \cite{radford2022robust} 
%TODO
\subsection{Seamless}
Seamless is a Multimodal model that uses a Transformer architecture. 
the architecture was proposed in \cite{communication2023seamlessm4t} 
%TODO

\subsection{DeltaLM}
DeltaLM is one of the current state of the art Neural Machine Translation models and architectures that was proposed in \cite{ma2021deltalm}. 
%TODO
