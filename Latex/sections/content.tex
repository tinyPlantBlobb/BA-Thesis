%% Karlsruhe Institute of Technology
%% Institute for Anthropomatics and Robotics (IAR)
%% Artificial Intelligence for Language Technologies (AI4LT) lab
%%
%% Prof. Dr. Jan Niehues
%% Lab's website https://ai4lt.anthropomatik.kit.edu/english/index.php


%TODO add methedology and move from experiments part



%TODO move stuff to the right places
\chapter{Translation and transctiptions}
Since all models that are worked with are Encoder-decoder-models that use the transformer architecture the resulting formulas for the resulting probabilities are simmilar, for completeness the probability formula per model is listed separately. 

\section{Whisper Transcription Probability}
Whisper \cite{radford2022robust} is a multitask and multilanguage model for Automatic speech recognition as well as speech translation. It is primarily used as an ASR model in this thesis. Open AI gives several different sizes for whisper models, in this thesis the medium model is used specifically the pretrained model that is available on huggingface. 


the Transcription Probability is calculated in a simmilar manner as the translation probability, which results in the formula $$-\frac{1}{T}\sum_{t=1}^T log p(y_t^v)$$ where T is the number of decoding steps and p is the probability 


\section{Translation Probability}
The translation probability is calcualted by the formula \ref{formula:translation probability} this is also the case in the seamless and deltalm model. 


\section{Seamless Translation probability}
The translation probability follows the general translation probability formula \ref{formula:translation probability}. To get the probability of the top token at each decoding step the huggingface api is used which is able to return the probabilities of all vocabulary entries 

The translation probabiltiy on seamless is jsut like the transcription probabilitiy on Whisper retrievel with the help of the pytorch and the seamless large model on huggingface. 


\section{DeltaLM translation probability}
the experiments \cite{ma2021deltalm} were done on a finetuned large version of it that was finetuned using the training data from the IWSLT 2023 constrained category, for this only the english german part was used
\\
the Text was preprocessed with the pretrained senctencepiece model and dictionary that has been given on the DeltaLM github page. 
the running of the experiments was done with the help of the fairseq toolkit \cite{ott2019fairseqfastextensibletoolkit}/\cite{ott2019fairseq}

\section{end-to-end translation probability}
In end-to-end translation there is no intermediate step between the audio and text part so the resulting probability is a single score for the whole process.
the resulting probability formula comes from the architecture, which in this case is a encoder decoder architecture which results in the formula: $$-\frac{1}{T}\sum_{t=1}^T log\; p(y_t)$$ which is the same as the translation probability and the transcription probability. 

\section{Softmax entropy}
in the formicheva \cite{fomicheva2020unsupervised} paper the Softmax entropy meassure was proposed as $$-\frac{1}{T}\sum_{t=1}^T\sum_{v=1}^Vp(y_t^v)logp(y_t^v) \label{formula:translation probability}$$ with V being the Vocabulary size and T being the length of the translation. Which in this definition doesn't work with the data that the models produce as there are quite a lot of 0 values for the vocabulary after using the softmax, which means that the result of the sum like that would not be defined. To mitigate this the 0 values are masked for the log and essentially ignored. 

to get those values from the seamless model basic code has been written that iterates over all vocabulary tokens and calculated the entropy of each decoding step
to get them from deltalm the fairseq source code was adapted to do the same
in both cases the batch and beam size are set to 1 as otherwise the resulting tensors make it more difficult to pick out which is the right entropy for this batch. 

%softmax entropy from the transcription part? maybe in the future 



\chapter{Dropout}
the dropout based metrics are obtained by doing 30 passes with the same input through the models each time neurons are masked to 0 my some probability, which is usually the same probability as was used in training. 

\section{Whisper Dropout}
for the dropout based uncertainty quantification the model transcribes the same bit of audio 30 times and for each pass the transcription probability and mean probability is computed, in each pass the model randomly masks some nodes to 0
this results in some short and some faulty transcriptions but this also gives information about how certain the model is in it's transcription. the used dropout probability is 0.1 


\section{Seamless Dropout}
for the dropout based approach the transcription with the best qe score, so the min/max score from the whisper step, is used 
and then put into seamless which then translates this 30 times with a dropout probabiltity of 0.1.
% potentially remove if finding a better solution
Due to the nature of pytorch and huggingface models the dropout has to be done in training mode, as the evaluation mode turns off any dropout layers that are in the model. Due to this and a bug in the implementation for caching during forward passes in the seamless model on huggingface, which leads to a tuple index out of range error that only appears in train mode with dropout turned on, the caching was turned off caching in the seamless config

\section{Deltalm Dropout}
The dropout with the deltaLM model is handled over fairseq. 
the used dropout probability is 0.1 and is applied on the Decoder and specically it's modules



\section{End-To-End seamless dropout}
The dropout for the end-to-end approach is once again applies on the decoder and as with the translation part of seamless the caching in the model has been turned off. The dropout probability is 0.1, same as in the translation part.


