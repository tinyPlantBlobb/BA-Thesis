
\chapter{Methodology}

\todo{how to derive the QE scores from the approaches like log probability or dropout,... }
\todo{all in a more mathe mathemical sende -> maybe move stuff from background here}
\section{Translation and transcription}
- take the softmax values at the last layer of the model to estimate quality (normal mode) for transctiption and transltation 
- to get the model values, aka probabilies at each decoding step the models return them during generation, hence why open source models, like Whisper, seamless and deltalm were used
\subsection{Entropy}
- get probabilities of whole vocabulary at decoding step, softmax it, compute the entropy\ref{entropy} for each element 
- sum all entropies together
- sum result of all decoding steps

\subsection{standard deviation}


\section{dropout}
\subsection{ general probability}
- run model with dropout several times and get the same values each run 
- sum over those values and take the average of those values as qe 
\subsection{variance}
- compute the variance between values of all the dropout runs 
- do this for transcription and translation 
%\subsection{lexsim}

%- for a single score in cascaded models: add/multiply the values together if need be with weights 
