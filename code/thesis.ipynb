{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import tarfile\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO exchange for the proper data set setup\n",
    "\n",
    "languages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\n",
    "selection = widgets.Dropdown(\n",
    "    options=[(\"Select language\", None), (\"----------\", None)] + sorted([(f\"{v} ({k})\", k) for k, v in languages.items()]),\n",
    "    value=\"ko_kr\",\n",
    "    description='Language:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "selection\n",
    "\n",
    "lang = selection.value\n",
    "language = languages[lang]\n",
    "\n",
    "assert lang is not None, \"Please select a language\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium\")\n",
    "options = dict(language=language, beam_size=5, best_of=5)\n",
    "transcribe_options = dict(task=\"transcribe\", **options)\n",
    "translate_options = dict(task=\"translate\", **options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO replace with proper dataset setup\n",
    "def download(url: str, target_path: str):\n",
    "    with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "\n",
    "class Fleurs(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, lang, split=\"test\", subsample_rate=1, device=DEVICE):\n",
    "        url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n",
    "        tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n",
    "        os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(tar_path):\n",
    "            download(url, tar_path)\n",
    "\n",
    "        all_audio = {}\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            for member in tar.getmembers():\n",
    "                name = member.name\n",
    "                if name.endswith(f\"{split}.tsv\"):\n",
    "                    labels = pd.read_table(tar.extractfile(member), names=(\"id\", \"file_name\", \"raw_transcription\", \"transcription\", \"_\", \"num_samples\", \"gender\"))\n",
    "\n",
    "                if f\"/{split}/\" in name and name.endswith(\".wav\"):\n",
    "                    audio_bytes = tar.extractfile(member).read()\n",
    "                    all_audio[os.path.basename(name)] = wavfile.read(io.BytesIO(audio_bytes))[1]                    \n",
    "\n",
    "        self.labels = labels.to_dict(\"records\")[::subsample_rate]\n",
    "        self.all_audio = all_audio\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        record = self.labels[item]\n",
    "        audio = torch.from_numpy(self.all_audio[record[\"file_name\"]].copy())\n",
    "        text = record[\"transcription\"]\n",
    "        \n",
    "        return (audio, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = 'WMT21_DA_test'\n",
    "data_root_path = '../data'\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'de'\n",
    "\n",
    "references = []\n",
    "transcriptions = []\n",
    "translations = []\n",
    "\n",
    "for audio, text in tqdm(dataset):\n",
    "    transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
    "    translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
    "    \n",
    "    transcriptions.append(transcription)\n",
    "    translations.append(translation)\n",
    "    references.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model with whisper medium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "options = dict(language=language, beam_size=5, best_of=5)\n",
    "transcribe_options = dict(task=\"transcribe\", **options)\n",
    "translate_options = dict(task=\"translate\", **options)\n",
    "references = []\n",
    "transcriptions = []\n",
    "translations = []\n",
    "\n",
    "# Returns the last layer proabbliities of the model as a dict containing the decoded text and the segments and the language\n",
    "\n",
    "for audio, text in tqdm(dataset):\n",
    "    transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
    "    translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
    "    \n",
    "    transcriptions.append(transcription)\n",
    "    translations.append(translation)\n",
    "    references.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(dict(reference=references, transcription=transcriptions, translation=translations))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the glass box info (in this case weights from the cross attention layers) from the models I will need Hooks to the layers. they are registered like this and will be called every time after the forward functuon is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QKs = [None] * model.dims.n_text_layer\n",
    "\n",
    "for i, block in enumerate(model.decoder.blocks):\n",
    "    block.cross_attn.register_forward_hook(\n",
    "        lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for the ASR part that are more than just hooks:\n",
    "also aka the translation probabaility approach expanded to the ASR part so the sequence level transcriptfosec.exchangeion probability normalised by audio length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcriptionProbability(tensor, **options):\n",
    "    return torch.nn.functional.softmax(tensor, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that are needed for the NMT part of the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardDeviation(probs):\n",
    "    return torch.sqrt(torch.mean(probs**2, dim=-1)-(torch.mean(probs, dim=-1) ** 2 ))\n",
    "\n",
    "# TODO fix formulas\n",
    "def translationProbabilities(tensor, **options):\n",
    "    with torch.no_grad():\n",
    "        # TODO add how to get the probability distribution that is generated by the model with the softmax function\n",
    "        probabilities = torch.nn.functional.softmax(tensor, dim=-1)\n",
    "        return probabilities\n",
    "    \n",
    "def softmaxEntropy(outputSequence):\n",
    "    probs = torch.nn.functional.softmax(outputSequence, dim=-1)\n",
    "    return -np.divide(1,len(outputSequence))* torch.sum(torch.sum(probs * torch.log(probs), dim=-1), dim=-1)\n",
    "\n",
    "\n",
    "def DropoutTranslationProbability(text, **options):\n",
    "    with torch.no_grad():\n",
    "        encoded = model.encode(text, **options)\n",
    "        logits = model.decode(encoded, **options)[\"logits\"]\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        dropout = torch.nn.Dropout(0.3) # as proposed by the paper\n",
    "        # 30 interference passes for the posteriour probabily but 10 should also be fine\n",
    "        probabilities = dropout(probabilities)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptionProbabilities =[]\n",
    "translationEntropy = []\n",
    "translationProbabilityList = []\n",
    "translationStandardDeviation = []\n",
    "#TODO exchange transcriptions for the correct list that contians the tensor of the last laysr or the translation proabblitiy for generatuing the sequence\n",
    "for i in transcriptions:\n",
    "    transcriptionProbabilities.append(transcriptionProbability(i))\n",
    "    translationProbabilityList.append(translationProbabilities(i))\n",
    "    translationEntropy.append(softmaxEntropy(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first 10 examples in the dataset\n",
    "for (audio, label), transcription in zip(dataset, transcriptions[:10]):\n",
    "    print(transcription)\n",
    "  \n",
    "    duration = len(audio)\n",
    "    mel = whisper.log_mel_spectrogram(whisper.pad_or_trim(audio)).cuda()\n",
    "    tokens = torch.tensor(\n",
    "        [\n",
    "            *tokenizer.sot_sequence,\n",
    "            tokenizer.timestamp_begin,\n",
    "        ] + tokenizer.encode(transcription) + [\n",
    "            tokenizer.timestamp_begin + duration // AUDIO_SAMPLES_PER_TOKEN,\n",
    "            tokenizer.eot,\n",
    "        ]\n",
    "    ).cuda()\n",
    "    with torch.no_grad():\n",
    "        logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))\n",
    "\n",
    "    weights = torch.cat(QKs)  # layers * heads * tokens * frames    \n",
    "    weights = weights[:, :, :, : duration // AUDIO_SAMPLES_PER_TOKEN].cpu()\n",
    "    weights = median_filter(weights, (1, 1, 1, medfilt_width))\n",
    "    weights = torch.tensor(weights * qk_scale).softmax(dim=-1)\n",
    "    \n",
    "    w = weights / weights.norm(dim=-2, keepdim=True)\n",
    "    matrix = w[-6:].mean(axis=(0, 1))\n",
    "\n",
    "    alignment = dtw(-matrix.double().numpy())\n",
    "\n",
    "    jumps = np.pad(np.diff(alignment.index1s), (1, 0), constant_values=1).astype(bool)\n",
    "    jump_times = alignment.index2s[jumps] * AUDIO_TIME_PER_TOKEN\n",
    "    words, word_tokens = split_tokens(tokens)\n",
    "\n",
    "    # display the normalized attention weights and the alignment\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(matrix, aspect=\"auto\")\n",
    "    plt.plot(alignment.index2s, alignment.index1s, color=\"red\")\n",
    "\n",
    "    xticks = np.arange(0, matrix.shape[1], 1 / AUDIO_TIME_PER_TOKEN)\n",
    "    xticklabels = (xticks * AUDIO_TIME_PER_TOKEN).round().astype(np.int32) \n",
    "    plt.xticks(xticks, xticklabels)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    \n",
    "    # display tokens and words as tick labels\n",
    "    ylims = plt.gca().get_ylim()\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.tick_params('both', length=0, width=0, which='minor', pad=6)\n",
    "\n",
    "    ax.yaxis.set_ticks_position(\"left\")\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylim(ylims)\n",
    "\n",
    "    major_ticks = [-0.5]\n",
    "    minor_ticks = []\n",
    "    current_y = 0\n",
    "    \n",
    "    for word, word_token in zip(words, word_tokens):\n",
    "        minor_ticks.append(current_y + len(word_token) / 2 - 0.5)\n",
    "        current_y += len(word_token)\n",
    "        major_ticks.append(current_y - 0.5)\n",
    "        \n",
    "    ax.yaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\n",
    "    ax.yaxis.set_minor_formatter(ticker.FixedFormatter(words))\n",
    "    ax.set_yticks(major_ticks)\n",
    "    ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "    \n",
    "    for label in ax.get_yminorticklabels():\n",
    "        label.set_fontproperties(prop)\n",
    "\n",
    "    plt.ylabel(\"Words\")\n",
    "    plt.show()\n",
    "\n",
    "    # display the word-level timestamps in a table\n",
    "    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n",
    "    begin_times = jump_times[word_boundaries[:-1]]\n",
    "    end_times = jump_times[word_boundaries[1:]]\n",
    "\n",
    "    data = [\n",
    "        dict(word=word, begin=begin, end=end)\n",
    "        for word, begin, end in zip(words[:-1], begin_times, end_times)\n",
    "        if not word.startswith(\"<|\") and word.strip() not in \".,!?、。\"\n",
    "    ]\n",
    "\n",
    "    display(pd.DataFrame(data))\n",
    "    display(HTML(\"<hr>\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
